# Quality/AQ/run_utmos.py
import os
import json
from pathlib import Path

# === å…³é”®ï¼šåœ¨å¯¼å…¥ utmosv2 ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ ===
# os.environ["HF_HUB_OFFLINE"] = "1"
# os.environ["TIMM_DISABLE_HF"] = "1"  # ğŸ‘ˆ ç¦ç”¨ timm çš„ Hugging Face é›†æˆ
# =========================================
from unittest.mock import patch

EFFNET_LOCAL_BIN = "/calc/users/cisri_shzh_gpu/users/lmy/asr/ASR_GT_pipline/Quality/AQ/timm/tf_efficientnetv2_s.in21k_ft_in1k/pytorch_model.bin"
W2V2_LOCAL_DIR = "/calc/users/cisri_shzh_gpu/users/lmy/asr/models/wav2vec2-base"  # ä½ çš„ wav2vec2 è·¯å¾„


def _offline_create_model(model_name, pretrained=False, **kwargs):
    """
    åŠ«æŒ timm.create_model å’Œ transformers AutoModel.from_pretrained
    """
    # 1. å¤„ç† EfficientNet-V2
    if model_name == "tf_efficientnetv2_s.in21k_ft_in1k" and pretrained:
        net = timm.create_model(model_name, pretrained=False, **kwargs)
        net.load_state_dict(torch.load(EFFNET_LOCAL_BIN, map_location="cpu"))
        return net

    # 2. å¤„ç† wav2vec2ï¼ˆtransformers ä¾§ï¼‰
    if model_name == "facebook/wav2vec2-base":
        from transformers import AutoModel, AutoFeatureExtractor

        processor = AutoFeatureExtractor.from_pretrained(
            W2V2_LOCAL_DIR, local_files_only=True
        )
        model = AutoModel.from_pretrained(W2V2_LOCAL_DIR, local_files_only=True)
        # è¿”å›æ¨¡å‹å¯¹è±¡ï¼ˆUTMOSv2 ä¼šè‡ªå·±å– modelï¼‰
        return model

    # å…¶ä½™æ¨¡å‹ä¿æŒé»˜è®¤
    return timm.create_model(model_name, pretrained=pretrained, **kwargs)


# 3. å…¨å±€æ‰“è¡¥ä¸ï¼ˆå¿…é¡»åœ¨ import utmosv2 ä¹‹å‰ï¼‰
patch("timm.create_model", side_effect=_offline_create_model).start()
patch(
    "transformers.AutoModel.from_pretrained", side_effect=_offline_create_model
).start()

import utmosv2  # å¿…é¡»åœ¨è®¾ç½®ç¯å¢ƒå˜é‡ä¹‹åå¯¼å…¥ï¼


def extract_utt_id(wav_path: str) -> str:
    return os.path.basename(wav_path).rsplit(".", 1)[0]


def process_directory(wav_dir: Path, output_jsonl: Path, model):
    print(f"ğŸ” Processing {wav_dir}")
    wav_files = sorted(wav_dir.glob("*.wav"))
    print(f"ğŸ“ Found {len(wav_files)} .wav files")

    results = []
    for i, wav_path in enumerate(wav_files):
        utt_id = extract_utt_id(str(wav_path))
        try:
            score = model.predict(input_path=str(wav_path))
            results.append({"utt_id": utt_id, "utmos": round(score, 4)})
        except Exception as e:
            print(f"âŒ Failed on {wav_path}: {e}")
            results.append({"utt_id": utt_id, "utmos": None})

        if (i + 1) % 50 == 0:
            print(f"  âœ… Processed {i+1}/{len(wav_files)}")

    with open(output_jsonl, "w", encoding="utf-8") as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"ğŸ‰ Done! Saved to {output_jsonl}\n")


def main():
    script_dir = Path(__file__).parent.resolve()
    checkpoint_path = script_dir / "UTMOSv2" / "fold0_s42_best_model.pth"

    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Model not found at {checkpoint_path}")

    # åˆ›å»ºæ¨¡å‹
    model = utmosv2.create_model(
        pretrained=True,
        config="fusion_stage3",
        fold=0,
        seed=42,
        checkpoint_path=str(checkpoint_path),
        device="auto",
    )

    base_data = Path("/calc/users/cisri_shzh_gpu/users/lmy/asr/ASR_GT_pipline/data")
    output_dir = script_dir

    process_directory(
        wav_dir=base_data / "dev" / "wavs",
        output_jsonl=output_dir / "dev_utmos.jsonl",
        model=model,
    )
    process_directory(
        wav_dir=base_data / "train_like" / "wavs",
        output_jsonl=output_dir / "train_like_utmos.jsonl",
        model=model,
    )


if __name__ == "__main__":
    main()
